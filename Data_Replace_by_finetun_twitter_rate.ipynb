{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Data Replace by finetun_twitter_rate.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "y8txwR-jMqB1",
        "uT0ahl3PMqB6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roger-G/Data-augmentation/blob/master/Data_Replace_by_finetun_twitter_rate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z98X5HJLMqBi",
        "colab_type": "text"
      },
      "source": [
        "### Import package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "Soj8UMxWMqBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "\"Fine-tuning BertMasked Model with labeled dataset\"\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import csv\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import shutil\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords \n",
        "from torch.utils.data import DataLoader, RandomSampler, TensorDataset, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import trange\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "PYTORCH_PRETRAINED_BERT_CACHE = Path(os.getenv('PYTORCH_PRETRAINED_BERT_CACHE',\n",
        "                                               Path.home() / '.pytorch_pretrained_bert'))\n",
        "# from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
        "from transformers import BertForMaskedLM\n",
        "from transformers import BertTokenizer\n",
        "logger = logging.getLogger(__name__)\n",
        "absFilePath = 'drive/My Drive/Data/Bert_ data augmentation-master/'\n",
        "clear_output()\n",
        "train_number = 0 ## this is made for differiate different augmentation data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp1sVnl1M1Oo",
        "colab_type": "code",
        "outputId": "a84fcbdf-a0f4-4216-9116-1643fd265585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFP35B8XMqBq",
        "colab_type": "text"
      },
      "source": [
        "### Class InputExample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "Nf-dN98vMqBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text = text\n",
        "        self.label = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h060gpUMqBw",
        "colab_type": "text"
      },
      "source": [
        "### Class AugProcessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "Y9ZLLWiCMqBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AugProcessor():\n",
        "  \n",
        "    \"\"\"Processor for dataset to be augmented.\"\"\"\n",
        "        \n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_csv(os.path.join(data_dir, \"train.tsv\")), \"train\") # get_train_examples\n",
        "\n",
        "    def get_dev_examples(self, data_dir): # get_dev_examples\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_csv(os.path.join(data_dir, \"dev.csv\")), \"dev\")\n",
        "\n",
        "    @ staticmethod\n",
        "    def get_labels(name): # get_labels\n",
        "        \"\"\"add your dataset here\"\"\"\n",
        "        if name in ['toxic', 'rt-polaritydata']:\n",
        "            return [\"0\", \"1\"]\n",
        "        else:\n",
        "            return ['positive', 'neutral', 'negative']\n",
        "          \n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            guid =\"%s-%s\" % (set_type, i)           \n",
        "            text = line[1][0]\n",
        "            label = line[1][-1]\n",
        "            # print('label', label)\n",
        "            examples.append(\n",
        "                InputExample(guid, text, label))\n",
        "        return examples\n",
        "\n",
        "    def _read_csv(self, input_file, quotechar='\"'):\n",
        "        \"\"\"Reads a comma separated value file.\"\"\"  \n",
        "        with open(input_file,\"r\",encoding='UTF-8') as f:\n",
        "            reader = csv.reader(\n",
        "                f,\n",
        "                delimiter=\",\",\n",
        "                quotechar=quotechar,\n",
        "                doublequote=True,\n",
        "                skipinitialspace=False,\n",
        "                )\n",
        "\n",
        "            lines = []\n",
        "            for index, line in enumerate(reader):\n",
        "              if index == 10:\n",
        "                  break\n",
        "              print(line)\n",
        "              line = ''.join(line).split('\\t')\n",
        "                # print('line', ''.join(line).split('\\t'))\n",
        "              lines.append((index,line))\n",
        "            # delete label and sentence\n",
        "            del lines[0]\n",
        "            # print('line', lines)\n",
        "        return lines\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8txwR-jMqB1",
        "colab_type": "text"
      },
      "source": [
        "### Convert_examples_to_features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wccnv55qMqB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def index_value(list_long, list_index, value):\n",
        "    for i in list_index:\n",
        "        list_long[i] = value\n",
        "    return list_long\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, masked_lm_prob):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "    # print('label_map', label_map)\n",
        "    features = []\n",
        "    dupe_factor = 5\n",
        "    rng = random.Random(123)\n",
        "    max_predictions_per_seq = 20\n",
        "    a = examples\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        # print('example', example)\n",
        "        tokens_a = tokenizer.tokenize(example.text) # (aaaaa, 1), aaaa is text\n",
        "        # print('tokens', type(tokens_a))\n",
        "        tokens_b = None\n",
        "        if len(tokens_a) > max_seq_length - 2:  # maxlength = [cls]+token_length + [sep]\n",
        "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "        # print('label', example.label, len(example.label))\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] # tokens_a is a token of sentence\n",
        "\n",
        "        pos = nltk.pos_tag(tokens) \n",
        "        index_vb = [ tokens.index(word) for word, tag in pos if tag in ('VBD', 'VB', 'VBG', 'VBN', 'JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNPS', 'NNP' ) \n",
        "                    and word not in stopwords.words('english') and word not in ('[CLS]', '[SEP]')]\n",
        "\n",
        "        s = example.label\n",
        "    \n",
        "        try:\n",
        "            label_id = label_map[s]\n",
        "        except KeyError:\n",
        "\n",
        "            label_id = label_map[s.strip()]\n",
        "        # print('lael_id', label_id)\n",
        "        segment_ids = [label_id] * len(tokens) # use this label to make label_id and segment_ids # here maybe is the contextual expression\n",
        "        masked_lm_labels = [0]*max_seq_length\n",
        "\n",
        "        cand_indexes = []\n",
        "        for (i, token) in enumerate(tokens):\n",
        "            if token == \"[CLS]\" or token == \"[SEP]\":\n",
        "                # print('i', i, token)\n",
        "                continue\n",
        "            cand_indexes.append(i)\n",
        "        rng.shuffle(cand_indexes) # 随机替换词为mask，为下面做准备\n",
        "        # print('cand_indexes', cand_indexes)\n",
        "        len_cand = len(cand_indexes)\n",
        "        output_tokens = list(tokens) ## contain '[CLS]' and '[SEP]'\n",
        "        num_to_predict = min(max_predictions_per_seq, ##  max_predictions_per_seq=20 最多预测20个\n",
        "                             max(1, int(round(len(tokens) * masked_lm_prob)))) # 最多替换tokens的50%\n",
        "        # print('num_to_predict', num_to_predict)\n",
        "        if len(index_vb) > num_to_predict:\n",
        "\n",
        "            index_vb_new = random.sample(index_vb, num_to_predict)\n",
        "            masked_lms_pos = index_vb_new\n",
        "            # print('masked_lms_pos_test', index_vb_new)\n",
        "            output_tokens = index_value(output_tokens, index_vb_new, '[MASK]')\n",
        "            masked_lm_labels = index_value(masked_lm_labels, index_vb_new, 1)\n",
        "            # print('output_tokens', output_tokens)\n",
        "            \n",
        "        else:\n",
        "            output_tokens = index_value(output_tokens, index_vb, '[MASK]')\n",
        "            masked_lms_pos = index_vb\n",
        "            # print('< enter else to add po one by one', masked_lms_pos)\n",
        "            covered_indexes = set(index_vb)\n",
        "            masked_lm_labels = index_value(masked_lm_labels, index_vb, 1)\n",
        "            for index in [indx for indx in cand_indexes if indx not in index_vb]: # cand_indexes 是除 [CLS] 和 ['SEP'] 的token 的index集合, 但是已经被shuffle了, sentence\n",
        "                if len(masked_lms_pos) >= num_to_predict:\n",
        "                    break\n",
        "                if index in covered_indexes:\n",
        "                    continue\n",
        "                covered_indexes.add(index)\n",
        "\n",
        "                # mask_pos = []\n",
        "                masked_token = None # 处理完一个词， should be reset for every token\n",
        "                # 80% of the time, replace with [MASK]，这个句子的token的80%换成[MASK], 10% keep original, 10% random \n",
        "\n",
        "                if rng.random() < 0.8:\n",
        "                    masked_token = \"[MASK]\"\n",
        "                        # output_tokens[index] = masked_token # mask 位置的token\n",
        "                    masked_lms_pos.append(index)\n",
        "                        # print('masked_lms_pos', masked_lms_pos)\n",
        "                    masked_lm_labels[index] = 1 #被mask的位置的原来的token的ids\n",
        "            \n",
        "                else:\n",
        "                    # 10% of the time, keep original\n",
        "                    # print('rng.random()', rng.random())\n",
        "                    if rng.random() < 0.5:\n",
        "                        masked_token = tokens[index]\n",
        "                    # 10% of the time, replace with random word\n",
        "                    else:\n",
        "                        masked_token = tokens[cand_indexes[rng.randint(0, len_cand - 1)]]\n",
        "                    # print('masked_token', masked_token)\n",
        "                output_tokens[index] = masked_token\n",
        "            \n",
        "        init_ids = tokenizer.convert_tokens_to_ids(tokens) # original tokens\n",
        "        token_idx = masked_lms_pos\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(output_tokens) # 80% of the time, replace with [MASK], token的80%probability 换成[MASK], 10% keep original, 10% random \n",
        "        input_mask = [1] * len(input_ids)\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        init_ids += padding\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(init_ids) == max_seq_length\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "            logger.info(\"init_ids: %s\" % \" \".join([str(x) for x in init_ids]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"masked_lm_labels: %s\" % \" \".join([str(x) for x in masked_lm_labels]))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(init_ids=init_ids, # original tokens ids\n",
        "                              input_ids=input_ids, # have mask in the sentences and to ids\n",
        "                              input_mask=input_mask, # padding 0 and no padding 1\n",
        "                              segment_ids=segment_ids, # if label=='1' then seg_ids = 1* [length of sentence ]\n",
        "                              masked_lm_labels=masked_lm_labels,\n",
        "                              token_idx=token_idx)) # 被mask的位置的原来的token的ids\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT0ahl3PMqB6",
        "colab_type": "text"
      },
      "source": [
        "### Class InputFeatures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "5mQJ2nTsMqB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, init_ids, input_ids, input_mask, segment_ids, masked_lm_labels, token_idx):\n",
        "        self.init_ids = init_ids\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.masked_lm_labels = masked_lm_labels\n",
        "        self.token_idx = token_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbbgr66sMqB_",
        "colab_type": "text"
      },
      "source": [
        "### Remove_wordpiece"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "dcV8gNfVMqCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_wordpiece(str):\n",
        "    if len(str) > 1:\n",
        "        for i in range(len(str) - 1, 0, -1):\n",
        "            if str[i] == '[PAD]':\n",
        "                str.remove(str[i])\n",
        "            elif len(str[i]) > 1 and str[i][0] == '#' and str[i][1] == '#':\n",
        "                str[i - 1] += str[i][2:]\n",
        "                str.remove(str[i])\n",
        "    return \" \".join(str[1:-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZW9h49JyY5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def choose_ids(pred_ids, origin_ids): # t=pre, m=origin\n",
        "  selected_ids = list()\n",
        "  for i in range(len(pred_ids)):\n",
        "\n",
        "    if origin_ids[i] == pred_ids[i][0]:\n",
        "      selected_ids.append(pred_ids[i][1].tolist())\n",
        "    else:\n",
        "      selected_ids.append(pred_ids[i][0].tolist())\n",
        "     \n",
        "  return selected_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgMOkJCyMqCD",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaEBMneqMqCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Train():\n",
        "\n",
        "  def __init__(self, train_numer, args, num_multi):\n",
        "    self.train_number = train_number, \n",
        "    self.num_multi = num_multi\n",
        "    self.args = args\n",
        "  \n",
        "  \n",
        "  def path_setting(self, task_name = 'twitter', tech_name = 'Bert_ data augmentation-master'):\n",
        "    abspath = '/content/drive/My Drive/Data/'\n",
        "    self.model_path = os.path.join(abspath, tech_name + '/save_model')\n",
        "    self.dataset_path = os.path.join(abspath, tech_name + '/datasets/{}/'.format(task_name))\n",
        "    self.save_path = os.path.join(abspath, tech_name + '/aug_data/{}/'.format(task_name))\n",
        "    \n",
        "    # return model_path, dataset_path, save_path\n",
        "  \n",
        "\n",
        "  @staticmethod\n",
        "  def load_model(model_name):\n",
        "    weights_path = os.path.join(self.model_path, model_name)\n",
        "    model = torch.load(weights_path)\n",
        "    return model\n",
        "\n",
        "  def multi(self):\n",
        "    task_name = self.args.task_name\n",
        "    self.path_setting()\n",
        "    self.args.data_dir = self.dataset_path\n",
        "    print('data_dir', self.args.data_dir)\n",
        "    self.save_train_path = os.path.join(self.save_path, \"{}_times_only_augmentation({})_{}.tsv\".format(self.num_multi, self.args.tune_state, train_number))\n",
        "    print('save_train_path', self.save_train_path)\n",
        "    self.origin_train_path = os.path.join(self.dataset_path, \"train.tsv\")\n",
        "    # print('self.origin_train_path', self.origin_train_path)\n",
        "    origin = pd.read_csv(self.origin_train_path,  error_bad_lines=False, sep='\\t')\n",
        "    data_len = len(origin)\n",
        "    origin_copy = origin.copy()\n",
        "  \n",
        "    for i, masked_lm_prob in enumerate([0.15, 0.20, 0.30]):\n",
        "      if i + 1 == self.num_multi:\n",
        "        break\n",
        "      '''loading model and do augmentation'''\n",
        "      save_path = self.Aug(self.train_number, masked_lm_prob, self.args, task_name, self.save_train_path, fine_tune=False)\n",
        "      new_file = pd.read_csv(save_path)\n",
        "      origin_copy = pd.concat([origin_copy, new_file[-data_len:]], ignore_index=True)\n",
        "\n",
        "    origin_copy.to_csv(self.save_path + '{}_times({})_{}.tsv'.format(self.num_multi, self.args.tune_state train_number), sep=\"\\t\", index=False)\n",
        "    pd.concat([new_file, origin], axis=1).to_csv(self.save_train_path, index=False)\n",
        "\n",
        "  @classmethod\n",
        "  def Aug(clf, train_number, masked_lm_prob, args, task_name, save_train_path):\n",
        "    processors = {\n",
        "        \"toxic\": AugProcessor,\n",
        "        'rt-polaritydata': AugProcessor,\n",
        "        'twitter': AugProcessor\n",
        "    }\n",
        "\n",
        "    processor = processors[task_name]()\n",
        "    label_list = processor.get_labels(task_name)\n",
        "    ''' change to hugging face version''' \n",
        "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case) \n",
        "    train_examples = None\n",
        "    num_train_steps = None\n",
        "    train_examples = processor.get_train_examples(args.data_dir)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print('device', device)\n",
        "\n",
        "    if args.tune_state = 'fine_tune':\n",
        "      cbert_name = args.model_name\n",
        "      model = clf.load_model(cbert_name)\n",
        "    else:\n",
        "      model = BertForMaskedLM.from_pretrained(args.bert_model)\n",
        "    model.to(device)\n",
        "    \n",
        "    # if task_name == 'stsa.fine':\n",
        "    model.bert.embeddings.token_type_embeddings = torch.nn.Embedding(3, 768)\n",
        "    model.bert.embeddings.token_type_embeddings.weight.data.normal_(mean=0.0, std=0.02)\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    global_step = 0\n",
        "    train_features = convert_examples_to_features(\n",
        "        train_examples, label_list, args.max_seq_length, tokenizer, masked_lm_prob)\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
        "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
        "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
        "\n",
        "    all_init_ids = torch.tensor([f.init_ids for f in train_features], dtype=torch.long)\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "    all_masked_lm_labels = torch.tensor([f.masked_lm_labels for f in train_features], dtype=torch.long)\n",
        "    train_data = TensorDataset(all_init_ids, all_input_ids, all_input_mask, all_segment_ids, all_masked_lm_labels)\n",
        "    train_dataloader = DataLoader(train_data,  batch_size=args.train_batch_size)\n",
        "    MASK_id = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
        "    \n",
        "    with open(save_train_path, 'a', encoding='UTF-8') as save_train_file:\n",
        "      tsv_writer = csv.writer(save_train_file, delimiter=',')\n",
        "      tsv_writer.writerow(['sentence', 'label'])\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # print('batch', batch)\n",
        "        init_ids, input_ids, input_mask, segment_ids, all_masked_lm_labels = batch\n",
        "        masked_idx = []\n",
        "        for i in all_masked_lm_labels.cpu().numpy():\n",
        "          s = np.nonzero(i)\n",
        "          masked_idx.append(s[0].tolist())\n",
        "        \n",
        "        predictions = model(input_ids, input_mask, segment_ids)\n",
        "               \n",
        "        for ids, idx, preds, seg in zip(init_ids, masked_idx, predictions[0], segment_ids):\n",
        "          # print('idx',idx)\n",
        "          # print('original token:',tokenizer.convert_ids_to_tokens(ids[idx].tolist()))\n",
        "          origin_ids = ids[idx]\n",
        "          # print('origi ids', origin_ids)\n",
        "          # idx = masked_idx[int(index)]\n",
        "          _, indice = torch.topk(torch.softmax(preds[idx], -1), 2)\n",
        "          selected_ids = choose_ids(indice, origin_ids)\n",
        "          # print('selected_ids', selected_ids)\n",
        "          ids[idx] = torch.LongTensor(selected_ids).to(device)   \n",
        "          pred_str = tokenizer.convert_ids_to_tokens(ids.cpu().numpy())\n",
        "          \n",
        "          pred_str = remove_wordpiece(pred_str)\n",
        "          # print('pred_str', pred_str)\n",
        "          tsv_writer.writerow([pred_str, seg[0].item()])\n",
        "\n",
        "    return save_train_path\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV_jFPgWMqCL",
        "colab_type": "text"
      },
      "source": [
        "## Parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anoUCaxIMqCM",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "\n",
        "def Parameter(train_number, num_multi):\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Required parameters\n",
        "    parser.add_argument(\"--data_dir\", default=\"datasets\", type=str,\n",
        "                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
        "    parser.add_argument(\"--output_dir\", default=\"aug_data\", type=str,\n",
        "                        help=\"The output dir for augmented dataset\")\n",
        "    parser.add_argument(\"--bert_model\", default=\"bert-base-uncased\", type=str,\n",
        "                        help=\"The path of pretrained bert model.\")\n",
        "    parser.add_argument(\"--model_name\", default=\"Fune-tune_BERT_twitter\", type=str,\n",
        "                    help=\"The name of fine-tuned bert model.\")\n",
        "    parser.add_argument(\"--task_name\",default=\"twitter\",type=str,\n",
        "                        help=\"The name of the task to train.\")\n",
        "    parser.add_argument(\"--tune_state\",default='unfine', type=str,\n",
        "                        help='To decide the state of loading model.')\n",
        "    parser.add_argument(\"--max_seq_length\", default=30, type=int,\n",
        "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
        "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
        "                             \"than this will be padded.\")\n",
        "    parser.add_argument(\"--do_lower_case\", default=True, action='store_true',\n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "    parser.add_argument(\"--train_batch_size\", default=32, type=int,\n",
        "                        help=\"Total batch size for training.\")\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "\n",
        "    parser.add_argument('--masked_lm_prob', type=int, default=0.25,\n",
        "                        help='the probabilty of MASK in original sentence')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "#     print(args.data_dir)\n",
        "    train = Train(train_number, args, num_multi)\n",
        "    train.multi()\n",
        "    # Train.Aug()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV70rmunJZaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import os.path as path\n",
        "df = pd.read_csv('/content/drive/My Drive/Data/Bert_ data augmentation-master/datasets/twitter/train.tsv', sep='\\t')\n",
        "train, test = train_test_split(df, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_Lmfal2JpQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.reset_index().drop(columns='index').to_csv(path.join('/content/drive/My Drive/Data/Bert_ data augmentation-master/datasets/twitter','train.tsv'), sep='\\t')\n",
        "test.reset_index().drop(columns='index').to_csv(path.join('/content/drive/My Drive/Data/Bert_ data augmentation-master/datasets/twitter','dev.tsv'), sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUDIYn77tCdo",
        "colab_type": "text"
      },
      "source": [
        "## Execute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0hyFLYEMqCQ",
        "colab_type": "code",
        "outputId": "52bbbe56-ba3a-47e8-fc37-995fd669dbec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  for i in range(2,5):\n",
        "    print('I want augment {} times'.format(i))\n",
        "    Parameter(train_number, i) # replace masked_lm_prob words\n",
        "    \n",
        "  train_number +=1\n",
        "\n",
        "  print('\\n','Finished!')\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I want augment 2 times\n",
            "data_dir /content/drive/My Drive/Data/Bert_ data augmentation-master/datasets/twitter/\n",
            "save_train_path /content/drive/My Drive/Data/Bert_ data augmentation-master/aug_data/twitter/2_times_only_augmentation(unfine)_0.tsv\n",
            "i 0\n",
            "['sentence\\tlabel']\n",
            "['win the match getin plus tomorrow be very busy day with awareness day and debate gulp debate\\tneutral']\n",
            "['some area of new england could see the first flake of the season tuesday\\tneutral']\n",
            "['nd bad qb definitely tony romo the man who like to share the ball with everyone include the other team\\tnegative']\n",
            "['thailand washington us president barack obama vow wednesday as he visit storm ravage new jersey shore to\\tneutral']\n",
            "['do you all hear what tony romo dress up as for halloween giants quaterback cause that be all he could throw to sunday night\\tneutral']\n",
            "['tim tebow may be availible wow jerry what the heck you wait for\\tpositive']\n",
            "['aga tayo tomorrow ah good night ces love you\\tpositive']\n",
            "['tina fey amy poehler be host the golden globe award on january what do you think\\tneutral']\n",
            "['lunch from my new lil spot the cotton bowl pretty goodsttimewill be go back\\tpositive']\n",
            "device cpu\n",
            "i 1\n",
            "I want augment 3 times\n",
            "data_dir /content/drive/My Drive/Data/Bert_ data augmentation-master/datasets/twitter/\n",
            "save_train_path /content/drive/My Drive/Data/Bert_ data augmentation-master/aug_data/twitter/3_times_only_augmentation(unfine)_0.tsv\n",
            "i 0\n",
            "['sentence\\tlabel']\n",
            "['win the match getin plus tomorrow be very busy day with awareness day and debate gulp debate\\tneutral']\n",
            "['some area of new england could see the first flake of the season tuesday\\tneutral']\n",
            "['nd bad qb definitely tony romo the man who like to share the ball with everyone include the other team\\tnegative']\n",
            "['thailand washington us president barack obama vow wednesday as he visit storm ravage new jersey shore to\\tneutral']\n",
            "['do you all hear what tony romo dress up as for halloween giants quaterback cause that be all he could throw to sunday night\\tneutral']\n",
            "['tim tebow may be availible wow jerry what the heck you wait for\\tpositive']\n",
            "['aga tayo tomorrow ah good night ces love you\\tpositive']\n",
            "['tina fey amy poehler be host the golden globe award on january what do you think\\tneutral']\n",
            "['lunch from my new lil spot the cotton bowl pretty goodsttimewill be go back\\tpositive']\n",
            "device cpu\n",
            "i 1\n",
            "['sentence\\tlabel']\n",
            "['win the match getin plus tomorrow be very busy day with awareness day and debate gulp debate\\tneutral']\n",
            "['some area of new england could see the first flake of the season tuesday\\tneutral']\n",
            "['nd bad qb definitely tony romo the man who like to share the ball with everyone include the other team\\tnegative']\n",
            "['thailand washington us president barack obama vow wednesday as he visit storm ravage new jersey shore to\\tneutral']\n",
            "['do you all hear what tony romo dress up as for halloween giants quaterback cause that be all he could throw to sunday night\\tneutral']\n",
            "['tim tebow may be availible wow jerry what the heck you wait for\\tpositive']\n",
            "['aga tayo tomorrow ah good night ces love you\\tpositive']\n",
            "['tina fey amy poehler be host the golden globe award on january what do you think\\tneutral']\n",
            "['lunch from my new lil spot the cotton bowl pretty goodsttimewill be go back\\tpositive']\n",
            "device cpu\n",
            "i 2\n",
            "I want augment 4 times\n",
            "data_dir /content/drive/My Drive/Data/Bert_ data augmentation-master/datasets/twitter/\n",
            "save_train_path /content/drive/My Drive/Data/Bert_ data augmentation-master/aug_data/twitter/4_times_only_augmentation(unfine)_0.tsv\n",
            "i 0\n",
            "['sentence\\tlabel']\n",
            "['win the match getin plus tomorrow be very busy day with awareness day and debate gulp debate\\tneutral']\n",
            "['some area of new england could see the first flake of the season tuesday\\tneutral']\n",
            "['nd bad qb definitely tony romo the man who like to share the ball with everyone include the other team\\tnegative']\n",
            "['thailand washington us president barack obama vow wednesday as he visit storm ravage new jersey shore to\\tneutral']\n",
            "['do you all hear what tony romo dress up as for halloween giants quaterback cause that be all he could throw to sunday night\\tneutral']\n",
            "['tim tebow may be availible wow jerry what the heck you wait for\\tpositive']\n",
            "['aga tayo tomorrow ah good night ces love you\\tpositive']\n",
            "['tina fey amy poehler be host the golden globe award on january what do you think\\tneutral']\n",
            "['lunch from my new lil spot the cotton bowl pretty goodsttimewill be go back\\tpositive']\n",
            "device cpu\n",
            "i 1\n",
            "['sentence\\tlabel']\n",
            "['win the match getin plus tomorrow be very busy day with awareness day and debate gulp debate\\tneutral']\n",
            "['some area of new england could see the first flake of the season tuesday\\tneutral']\n",
            "['nd bad qb definitely tony romo the man who like to share the ball with everyone include the other team\\tnegative']\n",
            "['thailand washington us president barack obama vow wednesday as he visit storm ravage new jersey shore to\\tneutral']\n",
            "['do you all hear what tony romo dress up as for halloween giants quaterback cause that be all he could throw to sunday night\\tneutral']\n",
            "['tim tebow may be availible wow jerry what the heck you wait for\\tpositive']\n",
            "['aga tayo tomorrow ah good night ces love you\\tpositive']\n",
            "['tina fey amy poehler be host the golden globe award on january what do you think\\tneutral']\n",
            "['lunch from my new lil spot the cotton bowl pretty goodsttimewill be go back\\tpositive']\n",
            "device cpu\n",
            "i 2\n",
            "['sentence\\tlabel']\n",
            "['win the match getin plus tomorrow be very busy day with awareness day and debate gulp debate\\tneutral']\n",
            "['some area of new england could see the first flake of the season tuesday\\tneutral']\n",
            "['nd bad qb definitely tony romo the man who like to share the ball with everyone include the other team\\tnegative']\n",
            "['thailand washington us president barack obama vow wednesday as he visit storm ravage new jersey shore to\\tneutral']\n",
            "['do you all hear what tony romo dress up as for halloween giants quaterback cause that be all he could throw to sunday night\\tneutral']\n",
            "['tim tebow may be availible wow jerry what the heck you wait for\\tpositive']\n",
            "['aga tayo tomorrow ah good night ces love you\\tpositive']\n",
            "['tina fey amy poehler be host the golden globe award on january what do you think\\tneutral']\n",
            "['lunch from my new lil spot the cotton bowl pretty goodsttimewill be go back\\tpositive']\n",
            "device cpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-a2acee242719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'I want augment {} times'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# replace masked_lm_prob words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mtrain_number\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-8043f6fdf170>\u001b[0m in \u001b[0;36mParameter\u001b[0;34m(train_number, num_multi)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#     print(args.data_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_multi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Train.Aug()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-2eaee3472a76>\u001b[0m in \u001b[0;36mmulti\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0;34m'''loading model and do augmentation'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m       \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_train_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfine_tune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m       \u001b[0mnew_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0morigin_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morigin_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdata_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-2eaee3472a76>\u001b[0m in \u001b[0;36mAug\u001b[0;34m(clf, train_number, masked_lm_prob, args, task_name, save_train_path, fine_tune)\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbert_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;31m# Instantiate model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertOnlyMLMHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_output_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36minit_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;34m\"\"\" Initialize and prunes weights if needed. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# Initialize weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;31m# Prune heads if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \"\"\"\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \"\"\"\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \"\"\"\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \"\"\"\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \"\"\"\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \"\"\"\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36m_init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;31m# Slightly different from the TF version which uses truncated_normal for initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;31m# cf https://github.com/pytorch/pytorch/pull/5617\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertLayerNorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JkRMG8IDlLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.tensor([i for i in range(100)], dtype=torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa3TQq7ePzs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class text():\n",
        "  d = 1\n",
        "  b = 3\n",
        "\n",
        "  def __init__(self):\n",
        "    self.d = d\n",
        "    self.b = b\n",
        "  def test(self):\n",
        "    print(self.d)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3zx4X_FZBc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = text()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mFQbyMkZMhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text.test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEo1RK8kZPyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
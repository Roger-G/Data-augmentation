{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Build_fine-tune_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roger-G/Data-augmentation/blob/master/Build_fine_tune_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSweZ99VpUOc",
        "colab_type": "text"
      },
      "source": [
        "## Import package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtYDczFk6l-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "!pip install pysnooper\n",
        "# !pip install torch\n",
        "\"Fine-tuning BertMasked Model with labeled dataset\"\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import csv\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, TensorDataset, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import trange\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from transformers import BertForMaskedLM\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AdamW\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt') \n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "absFilePath = 'drive/My Drive/Data/Bert_ data augmentation-master/'\n",
        "logger = logging.getLogger(__name__)\n",
        "train_number = 0\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LDtMQvS_TZ6",
        "colab_type": "code",
        "outputId": "b4d4f90f-6158-4b40-e778-ca952cb9e071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XyvKMW4pgzb",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NphhJXcG60An",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, init_ids, input_ids, input_mask, segment_ids, masked_lm_labels, token_idx):\n",
        "        self.init_ids = init_ids\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.masked_lm_labels = masked_lm_labels\n",
        "        self.token_idx = token_idx\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-vuBI8tpgJP",
        "colab_type": "text"
      },
      "source": [
        "## Conver_example_to_features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc8teuz17j0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def index_value(list_long, list_index, list_value):\n",
        "    if len(list_value) == 1:\n",
        "      for i in list_index:\n",
        "        list_long[i] = list_value[0]\n",
        "    else:\n",
        "      for i in list_index:\n",
        "        list_long[i] = list_value[i]\n",
        "    return list_long\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "    print('label_map', label_map)\n",
        "    features = []\n",
        "    dupe_factor = 5\n",
        "    masked_lm_prob = 0.15\n",
        "    rng = random.Random(123)\n",
        "    max_predictions_per_seq = 20\n",
        "    a = examples\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        tokens_a = tokenizer.tokenize(example.text_a) # (aaaaa, 1), aaaa is text_a\n",
        "        tokens_b = None\n",
        "        if len(tokens_a) > max_seq_length - 2:  # maxlength = [cls]+token_length + [sep]\n",
        "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "        # print('label', example.label, len(example.label))\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] # tokens_a is a token of sentence\n",
        "        # print('tokens', tokens)\n",
        "        #################################################################################\n",
        "        pos = nltk.pos_tag(tokens) \n",
        "        # print('pos', pos)\n",
        "        index_vb = [ind for ind, (word, tag) in enumerate(pos) if tag in ('VBD', 'VB', 'VBG', 'VBN', 'JJ', 'JJR', 'JJS') \n",
        "                    and (word not in stopwords.words('english')) and (word not in ('[CLS]', '[PAD]', '[SEP]'))]\n",
        "      \n",
        "        ######################################################################################            \n",
        "        s = example.label\n",
        "        try:\n",
        "            label_id = label_map[s]\n",
        "        except KeyError:\n",
        "\n",
        "            label_id = label_map[s.strip()]\n",
        "\n",
        "        segment_ids = [label_id] * len(tokens) # use this label to make label_id and segment_ids # here maybe is the contextual expression\n",
        "        masked_lm_labels = [-100]*max_seq_length\n",
        "        \n",
        "        # print('segment_ids', segment_ids)\n",
        "        cand_indexes = []\n",
        "        for (i, token) in enumerate(tokens):\n",
        "            if token not in [\"[CLS]\", '[PAD]', \"[SEP]\"]:\n",
        "              if token not in stopwords.words('english'):\n",
        "                cand_indexes.append(i)\n",
        "        rng.shuffle(cand_indexes) # 随机替换词为mask，为下面做准备\n",
        "        # print('cand_indexes', cand_indexes)\n",
        "        len_cand = len(cand_indexes)\n",
        "        output_tokens = list(tokens) ## contain '[CLS]' and '[SEP]'\n",
        "        num_to_predict = min(max_predictions_per_seq, ##  max_predictions_per_seq=20 最多预测20个\n",
        "                             max(1, int(round(len(tokens) * masked_lm_prob)))) # 最多替换tokens的50%\n",
        "        # print('num_to_predict', num_to_predict)\n",
        "        if len(index_vb) > num_to_predict:\n",
        "\n",
        "            index_vb_new = random.sample(index_vb, num_to_predict)\n",
        "            # print('masked_lms_pos_test', index_vb_new)\n",
        "            masked_lms_pos = index_vb_new\n",
        "            output_tokens = index_value(output_tokens, index_vb_new, ['[MASK]'])\n",
        "            all_tokens_ids = tokenizer.convert_tokens_to_ids(tokens) #被mask的位置的原来的token的ids\n",
        "            masked_lm_labels = index_value(masked_lm_labels, index_vb_new, all_tokens_ids)\n",
        "            \n",
        "            # token_idx[index] = tokenizer.convert_tokens_to_ids([tokens[index]])[0] #被mask的位置的原来的token的ids\n",
        "            # masked_origin_tokens = index_value(masked_lm_labels, index_vb_new, 1)\n",
        "            # print('output_tokens', output_tokens)\n",
        "            \n",
        "        else:\n",
        "            output_tokens = index_value(output_tokens, index_vb, ['[MASK]'])\n",
        "            masked_lms_pos = index_vb\n",
        "\n",
        "            covered_indexes = set(index_vb)\n",
        "            all_tokens_ids = tokenizer.convert_tokens_to_ids(tokens) \n",
        "            # print('all_tokens_ids', all_tokens_ids)\n",
        "            # all_tokens_ids = all_tokens_ids[0]\n",
        "            masked_lm_labels = index_value(masked_lm_labels, index_vb, all_tokens_ids)\n",
        "            for index in [indx for indx in cand_indexes if indx not in index_vb]: # cand_indexes 是除 [CLS] 和 ['SEP'] 的token 的index集合, 但是已经被shuffle了, sentence\n",
        "                if len(masked_lms_pos) >= num_to_predict:\n",
        "                    break\n",
        "                if index in covered_indexes:\n",
        "                    continue\n",
        "                covered_indexes.add(index)\n",
        "                masked_token = None # 处理完一个词， should be reset for every token\n",
        "                # 80% of the time, replace with [MASK]，这个句子的token的80%换成[MASK], 10% keep original, 10% random \n",
        "                if rng.random() < 0.8:\n",
        "                    masked_token = \"[MASK]\"\n",
        "                    # output_tokens[index] = masked_token # mask 位置的token\n",
        "                    masked_lms_pos.append(index)\n",
        "                    # print('masked_lms_pos', masked_lms_pos)\n",
        "                    masked_lm_labels[index] = tokenizer.convert_tokens_to_ids([tokens[index]])[0]  #被mask的位置的原来的token的ids\n",
        "                else:\n",
        "                    # 10% of the time, keep original\n",
        "                    # print('rng.random()', rng.random())\n",
        "                    if rng.random() < 0.5:\n",
        "                        masked_token = tokens[index]\n",
        "                    # 10% of the time, replace with random word\n",
        "                    else:\n",
        "                        masked_token = tokens[cand_indexes[rng.randint(0, len_cand - 1)]]\n",
        "                    # print('masked_token', masked_token)\n",
        "                output_tokens[index] = masked_token\n",
        "\n",
        "                \n",
        "            # print('masked_lm_labels and ', masked_lm_labels)\n",
        "            # output_tokens[index] = masked_token # mask 位置的token\n",
        "            # masked_lms_pos.append(index) ## 被mask的token 在这句话的位置\n",
        "        \n",
        "        # print('masked_lm_labels', masked_lm_labels)\n",
        "        # print('original tokens before training', tokens)\n",
        "        init_ids = tokenizer.convert_tokens_to_ids(tokens) # original tokens\n",
        "        token_idx = masked_lms_pos # token de position\n",
        "        # print('output_tokens', output_tokens, type(output_tokens))\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(output_tokens) # ids : original tokens with [MASK] \n",
        "        input_mask = [1] * len(input_ids) # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        init_ids += padding\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(init_ids) == max_seq_length\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "        # print('init_ids', init_ids)\n",
        "        # print('input_ids', input_ids)\n",
        "        # print('input_mask', input_mask)\n",
        "        # print('segment_ids', segment_ids)\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                [str(x) for x in tokens]))\n",
        "            logger.info(\"init_ids: %s\" % \" \".join([str(x) for x in init_ids]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\n",
        "                \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"masked_lm_labels: %s\" % \" \".join([str(x) for x in masked_lm_labels]))\n",
        "        features.append(\n",
        "                InputFeatures(init_ids=init_ids, # original tokens ids\n",
        "                              input_ids=input_ids, # have mask in the sentences and to ids\n",
        "                              input_mask=input_mask, # padding 0 and no padding 1\n",
        "                              segment_ids=segment_ids, # if label=='1' then seg_ids = 1* [length of sentence ]\n",
        "                              masked_lm_labels=masked_lm_labels, # 做label用， 这样才能去训练\n",
        "                              token_idx=token_idx)) # 被mask的位置的原来的token的ids\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3ZhOpA23ZKR",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DnikmKf7n06",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def main(train_number):\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Required parameters\n",
        "    parser.add_argument(\"--data_dir\", default=\"datasets\", type=str,\n",
        "                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
        "    parser.add_argument(\"--output_dir\", default=\"aug_data\", type=str,\n",
        "                        help=\"The output dir for augmented dataset\")\n",
        "    parser.add_argument(\"--bert_model\", default=\"bert-base-uncased\", type=str,\n",
        "                        help=\"The path of pretrained bert model.\")\n",
        "    parser.add_argument(\"--task_name\",default=\"twitter\",type=str,\n",
        "                        help=\"The name of the task to train.\")\n",
        "    parser.add_argument(\"--max_seq_length\", default=30, type=int,\n",
        "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
        "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
        "                             \"than this will be padded.\")\n",
        "    parser.add_argument(\"--do_lower_case\", default=True, action='store_true',\n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "    parser.add_argument(\"--train_batch_size\", default=4, type=int,\n",
        "                        help=\"Total batch size for training.\")\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--num_train_epochs\", default=10, type=float,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--warmup_proportion\", default=0.1, type=float,\n",
        "                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
        "                             \"E.g., 0.1 = 10%% of training.\")\n",
        "    parser.add_argument('--seed', type=int, default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    # print(args.data_dir)\n",
        "    run_aug(train_number, args, save_every_epoch=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J68UkGmY62A4",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class AugProcessor(DataProcessor):\n",
        "  \"\"\"Processor for dataset to be augmented.\"\"\"\n",
        "\n",
        "  def get_train_examples(self, data_dir):\n",
        "      \"\"\"See base class.\"\"\"\n",
        "      print('data_dir in augprocessor', data_dir)\n",
        "      return self._create_examples(\n",
        "          self._read_csv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "  def get_dev_examples(self, data_dir):\n",
        "      \"\"\"See base class.\"\"\"\n",
        "      return self._create_examples(\n",
        "          self._read_csv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
        "\n",
        "  def get_labels(self, name):\n",
        "      \"\"\"add your dataset here\"\"\"\n",
        "      if name in ['toxic']:\n",
        "          return [\"0\", \"1\"]\n",
        "      else:\n",
        "          return ['positive', 'negative', 'neutral']\n",
        "\n",
        "  def _create_examples(self, lines, set_type):\n",
        "      \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "      examples = []\n",
        "      for (i, line) in enumerate(lines):\n",
        "          guid =\"%s-%s\" % (set_type, i)\n",
        "          text_a = line[1][0]\n",
        "          text_b = None\n",
        "          label = line[1][-1]\n",
        "          examples.append(\n",
        "              InputExample(guid, text_a, text_b, label))\n",
        "      # print('examples:', examples)\n",
        "      return examples\n",
        "\n",
        "  @classmethod\n",
        "  def _read_csv(cls, input_file, quotechar='\"'):\n",
        "      \"\"\"Reads a comma separated value file.\"\"\"  \n",
        "      print('input_file', input_file)\n",
        "      with open(input_file,\"r\",encoding='UTF-8') as f:\n",
        "          # print('input-file', input_file)\n",
        "          reader = csv.reader(\n",
        "              f,\n",
        "              delimiter=\",\",\n",
        "              quotechar=quotechar,\n",
        "              doublequote=True,\n",
        "              skipinitialspace=False,\n",
        "              )\n",
        "\n",
        "          lines = []\n",
        "          for index, line in enumerate(reader):\n",
        "              # if index == 500:\n",
        "              #     break\n",
        "                # print(line)\n",
        "              line = ''.join(line).split('\\t')\n",
        "                # print('line', ''.join(line).split('\\t'))\n",
        "              lines.append((index,line))\n",
        "          del lines[0]\n",
        "          # print('line', lines)\n",
        "      print('length of data', len(lines))\n",
        "      return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH67ky0c64NS",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "\n",
        "def remove_wordpiece(str):\n",
        "    if len(str) > 1:\n",
        "        for i in range(len(str) - 1, 0, -1):\n",
        "            if str[i] == '[PAD]':\n",
        "                str.remove(str[i])\n",
        "            elif len(str[i]) > 1 and str[i][0] == '#' and str[i][1] == '#':\n",
        "                str[i - 1] += str[i][2:]\n",
        "                str.remove(str[i])\n",
        "    return \" \".join(str[1:-1])\n",
        "\n",
        "\n",
        "\n",
        "def path_setting(task_name = 'twitter', tech_name = 'Bert_ data augmentation-master'):\n",
        "  abspath = '/content/drive/My Drive/Data/'\n",
        "  model_path = os.path.join(abspath, tech_name + '/save_model')\n",
        "  dataset_path = os.path.join(abspath, tech_name + '/datasets/{}/'.format(task_name))\n",
        "  save_path = os.path.join(abspath, tech_name + '/aug_data/{}/'.format(task_name))\n",
        "  return model_path, dataset_path, save_path\n",
        "\n",
        "# train_number = 0\n",
        "# train_number += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI4TQoWrp2qy",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj6qSc7B66UP",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "3d775c86-0b56-496c-daf6-79118dcc3485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "def run_aug(train_number, args, save_every_epoch=False):\n",
        "    # Augment the dataset with your own choice of Processer\n",
        "    processors = {\n",
        "        \"toxic\": AugProcessor,\n",
        "        'twitter': AugProcessor\n",
        "    }\n",
        "\n",
        "    task_name = args.task_name\n",
        "    if task_name not in processors:\n",
        "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
        "\n",
        "    model_path, dataset_path, save_path = path_setting(task_name=task_name)\n",
        "    args.data_dir = dataset_path\n",
        "    print('data_dir', type(args.data_dir))\n",
        "    args.output_dir = save_path\n",
        "    print('output_dir', args.output_dir)\n",
        "\n",
        "    processor = processors[task_name]()\n",
        "    label_list = processor.get_labels(task_name)\n",
        "    print('label_list', label_list)\n",
        "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case) \n",
        "\n",
        "    train_examples = None\n",
        "    num_train_steps = None\n",
        "    print('args.data_dir', args.data_dir)\n",
        "    train_examples = processor.get_train_examples(args.data_dir)\n",
        "    \n",
        "    #dev_examples = processor.get_dev_examples(args.data_dir)\n",
        "    #train_examples.extend(dev_examples)\n",
        "    num_train_steps = int(len(train_examples) / args.train_batch_size * args.num_train_epochs) \n",
        "    \n",
        "    # scheduler = WarmupCosineWithHardRestartsSchedule(optimizer=optimizer, warmup_steps=WARMUP_STEPS, t_total=-1)\n",
        "    #  optimizer = BertAdam(optimizer_grouped_parameters,lr=args.learning_rate,\n",
        "                        #  warmup=args.warmup_proportion,t_total=t_total)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print('device', device)\n",
        "    #  MODEL_name = \"{}/BertForMaskedLM_aug{}_epoch_3\".format(task_name.lower(), task_name.lower())\n",
        "    model = BertForMaskedLM.from_pretrained(args.bert_model)\n",
        "    \n",
        "    \n",
        "    model.bert.embeddings.token_type_embeddings = torch.nn.Embedding(3, 768)\n",
        "    model.bert.embeddings.token_type_embeddings.weight.data.normal_(mean=0.0, std=0.02)\n",
        "    \n",
        "    model.to(device)\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
        "    global_step = 0\n",
        "    train_features = convert_examples_to_features(\n",
        "        train_examples, label_list, args.max_seq_length, tokenizer)\n",
        "    # clear_output()\n",
        "    print('the training example')\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
        "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
        "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
        "\n",
        "    all_init_ids = torch.tensor([f.init_ids for f in train_features], dtype=torch.long)\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "\n",
        "    all_masked_lm_labels = torch.tensor([f.masked_lm_labels for f in train_features], dtype=torch.long)\n",
        "    train_data = TensorDataset(all_init_ids, all_input_ids, all_input_mask, all_segment_ids, all_masked_lm_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "    MASK_id = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
        "    save_model_dir = model_path\n",
        "    model.train()\n",
        "\n",
        "    for e in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
        "      best_accuracy = 0\n",
        "      avg_loss = 0\n",
        "        # torch.cuda.empty_cache()\n",
        "      count = 0\n",
        "        # shutil.copy(origin_train_path, save_train_path)\n",
        "        \n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        init_ids, input_ids, input_mask, segment_ids, all_masked_lm_labels = batch\n",
        "        loss = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask, masked_lm_labels=all_masked_lm_labels)[0]\n",
        "        # print('loss', loss.item())\n",
        "        loss.backward()\n",
        "        avg_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "\n",
        "      print('avg_loss: {}'.format(avg_loss / len(train_dataloader)))      \n",
        "    save_model_name = \"Fune-tune_BERT_\" + task_name\n",
        "    save_model_path = os.path.join(save_model_dir, save_model_name)\n",
        "                # print('where is the model', save_model_path)\n",
        "    torch.save(model, save_model_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
            "Wall time: 11.9 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9ZDU6c268JR",
        "colab_type": "code",
        "outputId": "744cecb9-e36a-44e3-ccf6-91c4262dd005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main(train_number)\n",
        "  train_number +=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_dir <class 'str'>\n",
            "output_dir /content/drive/My Drive/Data/Bert_ data augmentation-master/aug_data/twitter/\n",
            "label_list ['positive', 'negative', 'neutral']\n",
            "args.data_dir /content/drive/My Drive/Data/Bert_ data augmentation-master/datasets/twitter/\n",
            "data_dir in augprocessor /content/drive/My Drive/Data/Bert_ data augmentation-master/datasets/twitter/\n",
            "input_file /content/drive/My Drive/Data/Bert_ data augmentation-master/datasets/twitter/train.tsv\n",
            "length of data 41705\n",
            "device cuda:0\n",
            "label_map {'positive': 0, 'negative': 1, 'neutral': 2}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "the training example\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  10%|█         | 1/10 [12:51<1:55:38, 771.00s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 4.443307448453273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  20%|██        | 2/10 [25:52<1:43:13, 774.17s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 2.6607567715971525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  30%|███       | 3/10 [38:54<1:30:34, 776.35s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 1.2026377907443524\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  40%|████      | 4/10 [51:47<1:17:33, 775.54s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 0.47995417502327703\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  50%|█████     | 5/10 [1:04:45<1:04:40, 776.08s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 0.30567674304800696\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  60%|██████    | 6/10 [1:17:45<51:49, 777.29s/it]  \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 0.2483018021949145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  70%|███████   | 7/10 [1:30:39<38:49, 776.39s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 0.21910104118511306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  80%|████████  | 8/10 [1:43:50<26:01, 780.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 0.19854988954524802\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  90%|█████████ | 9/10 [1:56:55<13:01, 781.96s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 0.18293199704937235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 100%|██████████| 10/10 [2:09:52<00:00, 779.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "avg_loss: 0.17232712157343683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
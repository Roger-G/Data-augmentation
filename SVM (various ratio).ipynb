{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LR.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r1QOU8tv2Bmt",
        "9trl8M2u2qSD",
        "ZCDHsUBFilbq",
        "N3Tye3Op3wyP",
        "qDAIH9G9YJpN",
        "rtGzvkCHhRFP",
        "CRp469Y3hYoY"
      ],
      "mount_file_id": "1ePOz_2kBIZoHlIbnZFuweZfUY7rt2z84",
      "authorship_tag": "ABX9TyOsOhvwvkNVJHNF0ZISZiVz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roger-G/Data-augmentation/blob/master/SVM%20(various%20ratio).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPBGt0UZ0rhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# df_train = pd.read_csv('/content/drive/My Drive/Data/Back_trans/datasets/twitter/train.tsv', sep = '\\t')\n",
        "# print('info', df_train.info())\n",
        "# print('value_counts', df_train.label.value_counts())\n",
        "# ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# ! unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTK66QYAAlfx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ef4f92d0-2f13-4b7e-f330-64b30acc7914"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import seaborn as sns\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import csv\n",
        "import torch.utils.data as Data\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') \n",
        "from sklearn.svm import LinearSVC\n",
        "from tqdm import trange\n",
        "import pandas as pd\n",
        "import torchtext.vocab as Vocab\n",
        "import collections\n",
        "# with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
        "#     w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
        "#            for line in lines}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adb0dXYR1Oo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn import svm\n",
        "import os.path as path\n",
        "import re\n",
        "from sklearn.utils import resample\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk6TYPWJ9Hxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preprocessor():\n",
        "    \n",
        "  def __init__(self, data, max_seq_len, vocab):\n",
        "    self.data = data\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.vocab = vocab\n",
        "    self.PAD, self.BOS, self.EOS = '<pad>', '<bos>', '<eos>'\n",
        "    self.all_tokens = []\n",
        "    self.all_seqs = []\n",
        "\n",
        "  def process_one_seq(self, seq_tokens):\n",
        "    self.all_tokens.extend(seq_tokens) # add seq_tokens into one list, like extend the list, [..., seq_tokens].\n",
        "    seq_tokens = [self.PAD] * (self.max_seq_len - len(seq_tokens)) + seq_tokens\n",
        "    self.all_seqs.append(seq_tokens) # add seq_tokens become a list element, [...,[seq_tokens]].\n",
        "      \n",
        "  def build_data(self):\n",
        "    if self.vocab:\n",
        "      indices = [[self.vocab.stoi[w] for w in seq] for seq in self.all_seqs] # use its vocab_dic to represent the sentence. \n",
        "    else:\n",
        "      tokens_dic = collections.Counter(self.all_tokens) \n",
        "      self.vocab = Vocab.Vocab(tokens_dic, specials=[self.PAD])\n",
        "      indices = [[self.vocab.stoi[w] for w in seq] for seq in self.all_seqs] # use its vocab_dic to represent the sentence. \n",
        "    return indices\n",
        "  \n",
        "  @staticmethod\n",
        "  def normalizeString(s):\n",
        "    s = s.str.lower()\n",
        "    s = s.str.replace(r\"<br />\",r\" \")\n",
        "    s = s.str.replace(r'(\\W)(?=\\1)', '')\n",
        "    s = s.str.replace(r\"([.!?])\", r\" \\1\")\n",
        "    s = s.str.replace(r\"[^a-zA-Z.!?]+\", r\" \")\n",
        "    return s\n",
        "\n",
        "  def read_data(self):\n",
        "    df = self.data.copy()\n",
        "    df = df.dropna()\n",
        "    df['sentence'] = self.normalizeString(df['sentence'])\n",
        "    df.reset_index(inplace=True, drop=True)\n",
        "    k = len(df)\n",
        "    target = []\n",
        "\n",
        "    for line in range(k):\n",
        "      in_seq_tokens = df['sentence'][line].split(' ')\n",
        "      if len(in_seq_tokens) > self.max_seq_len - 1:\n",
        "        continue\n",
        "      self.process_one_seq(in_seq_tokens)\n",
        "      target.append(df.label[line])\n",
        "\n",
        "    in_data = self.build_data() # in_tokens is the list where contains every word, in_seqs is a list where its element are the sentence in French.\n",
        "\n",
        "    return [in_data, target]\n",
        "    # Data.TensorDataset(in_data, torch.tensor(target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDyGlgVu1jxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Loader():\n",
        "\n",
        "\n",
        "  def path_setting(self, task_name, tech_name):\n",
        "    abspath = '/content/drive/My Drive/Data/'\n",
        "    model_path = path.join(abspath, tech_name + '/save_model')\n",
        "    dataset_path = path.join(abspath, tech_name + '/datasets/{}/'.format(task_name))\n",
        "    save_path = path.join(abspath, tech_name + '/aug_data/{}/'.format(task_name))\n",
        "    \n",
        "    return model_path, dataset_path, save_path\n",
        "\n",
        "  @staticmethod\n",
        "  def load_data(path_, FILE_NAME, ratio, tech_name):\n",
        "\n",
        "    df_train = pd.read_csv(os.path.join(path_,'train.tsv'), sep='\\t',  error_bad_lines=False)\n",
        "    df_train = df_train.dropna()\n",
        "\n",
        "    num = int(len(df_train) * ratio)\n",
        "    df_train = df_train[ : num ]\n",
        "    num_DA = max(df_train.label.value_counts()) - min(df_train.label.value_counts())\n",
        "\n",
        "    if FILE_NAME != 'oversampling':\n",
        "      df = pd.read_csv(os.path.join(path_, FILE_NAME + '.tsv'), sep='\\t',  error_bad_lines=False)    \n",
        "      df = df.dropna()\n",
        "      if FILE_NAME == 'train' or tech_name == 'Back_trans':\n",
        "        df = df[: num ]\n",
        "    \n",
        "    else:\n",
        "      # 选出差额， 补到原数据上面\n",
        "      df_over_samp = resample(df_train[df_train.label==0], n_samples=num_DA, random_state=100, replace=True)\n",
        "      df = pd.concat([df_over_samp, df_train])\n",
        "\n",
        "    if FILE_NAME != 'train' and FILE_NAME != 'dev' and FILE_NAME != 'oversampling' and FILE_NAME != 'test':\n",
        "      try:\n",
        "        df_over_samp = resample(df, n_samples=num_DA, replace = False, random_state=10) \n",
        "      except ValueError:\n",
        "        df_over_samp = resample(df, n_samples=num_DA, replace = True, random_state=10) \n",
        "\n",
        "      df = pd.concat([df_train, df_over_samp]) \n",
        "    \n",
        "    else: \n",
        "      pass\n",
        "\n",
        "    df = df.sample(frac = 1, replace=False, random_state=102).reset_index().drop(columns = ['index'])\n",
        "    return df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc7n1TZ71dH5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "#@\n",
        "\n",
        "\n",
        "\n",
        "def lr_cv(splits, X, Y, pipeline, average_method):\n",
        "    dev_data = pd.read_csv('/content/drive/My Drive/Data/Back_trans/aug_data/twitter/dev.tsv', sep='\\t')\n",
        "    dev_data.dropna(inplace=True)\n",
        "    x_dev, y_dev = dev_data.sentence, dev_data.label\n",
        "    kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=777)\n",
        "    accuracy = []\n",
        "    precision = []\n",
        "    recall = []\n",
        "    f1 = []\n",
        "    for train, test in kfold.split(X, Y):\n",
        "        lr_fit = pipeline.fit(X[train], Y[train])\n",
        "        prediction = lr_fit.predict(X[test])\n",
        "        scores = lr_fit.score(X[test],Y[test])\n",
        "        \n",
        "        accuracy.append(scores * 100)\n",
        "        precision.append(precision_score(Y[test], prediction, average=average_method)*100)\n",
        "        # print('first precision', precision)\n",
        "        print('              negative    neutral     positive')\n",
        "        print('precision:',precision_score(Y[test], prediction, average=None))\n",
        "        recall.append(recall_score(Y[test], prediction, average=average_method)*100)\n",
        "        print('recall:   ',recall_score(Y[test], prediction, average=None))\n",
        "        f1.append(f1_score(Y[test], prediction, average=average_method)*100)\n",
        "        print('f1 score: ',f1_score(Y[test], prediction, average=None))\n",
        "        print('-'*50)\n",
        "\n",
        "    prediction_dev = lr_fit.predict(x_dev)\n",
        "    dev_accuracy = lr_fit.score(x_dev, y_dev)\n",
        "\n",
        "\n",
        "    print(\"accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(accuracy), np.std(accuracy)))\n",
        "    print(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision), np.std(precision)))\n",
        "    print(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall), np.std(recall)))\n",
        "    print(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1), np.std(f1)))\n",
        "    print('*'* 50)\n",
        "    precision = precision_score(y_dev, prediction_dev, average=average_method)\n",
        "    # print('precision', precision)\n",
        "    recall = recall_score(y_dev, prediction_dev, average=average_method)\n",
        "    f1 = f1_score(y_dev, prediction_dev, average=average_method)\n",
        "    print(\"accuracy: %.2f\" % (dev_accuracy))\n",
        "    print(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision), np.std(precision)))\n",
        "    print(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall), np.std(recall)))\n",
        "    print(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1), np.std(f1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpFS-kWR9Z4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_matrix(data, pipeline, average_method, save_path):\n",
        "\n",
        "  dev_data = pd.read_csv(path.join(save_path, 'test.tsv'), sep='\\t')\n",
        "  dev_data.dropna(inplace=True)\n",
        "  x_dev, y_dev = dev_data.sentence, dev_data.label\n",
        "  x_train, y_train = data.sentence, data.label\n",
        "  lr_fit = pipeline.fit(x_train, y_train)\n",
        "  prediction_dev = lr_fit.predict(x_dev)\n",
        "  dev_accuracy = lr_fit.score(x_dev, y_dev)\n",
        "  precision = precision_score(y_dev, prediction_dev, average=average_method)\n",
        "  # print('precision', precision)\n",
        "  recall = recall_score(y_dev, prediction_dev, average=average_method)\n",
        "  f1 = f1_score(y_dev, prediction_dev, average=average_method)\n",
        "\n",
        "  print(\"accuracy: %.4f\" % (dev_accuracy))\n",
        "  print(\"precision: %.4f\" % (np.mean(precision)))\n",
        "  print(\"recall: %.4f\" % (np.mean(recall)))\n",
        "  print(\"f1 score: %.4f\" % (np.mean(f1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97klAzwG6Xf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# class excuter():\n",
        "def pipelines(tkz, clf):\n",
        "\n",
        "  ROS_pipeline = make_pipeline(tkz, RandomOverSampler(random_state=777),clf)\n",
        "  SMOTE_pipeline = make_pipeline(tkz, SMOTE(random_state=777),clf)\n",
        "  ADASYN_pipeline = make_pipeline(tkz, ADASYN(ratio='minority',random_state=777), clf)\n",
        "\n",
        "  original_pipeline = Pipeline([\n",
        "      ('vectorizer', tkz),\n",
        "      ('classifier', clf)\n",
        "  ])\n",
        "  return ROS_pipeline, original_pipeline\n",
        "\n",
        "def execute(test_list, ratio, task_name, tech_name, tkz, clf):\n",
        "\n",
        "  loader = Loader()\n",
        "  \n",
        "  model_path, dataset_path, save_path = loader.path_setting(task_name, tech_name)\n",
        "  ROS_pipeline, original_pipeline = pipelines(tkz, clf)\n",
        "  for file_name in test_list:\n",
        "\n",
        "    print('-' * 70)\n",
        "    print('Start to make classification use the data : {}'.format(file_name))\n",
        "    print('-' * 70)\n",
        "    # DA and oversampling data\"\n",
        "    df_train = loader.load_data(save_path, file_name, ratio, tech_name)\n",
        "    if file_name == 'oversampling':\n",
        "      df_train = loader.load_data(save_path, 'train', ratio, tech_name)\n",
        "      eval_matrix(df_train, ROS_pipeline, 'macro', save_path)\n",
        "\n",
        "    else:\n",
        "      eval_matrix(df_train, original_pipeline, 'macro', save_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1QOU8tv2Bmt",
        "colab_type": "text"
      },
      "source": [
        "## Back_trans "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCDHsUBFilbq",
        "colab_type": "text"
      },
      "source": [
        "### SVM  new test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUg_ylkeuw0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8d05e4d0-5145-4f2f-b744-e3587f13fbe9"
      },
      "source": [
        "\n",
        "tvec = TfidfVectorizer(stop_words=None, max_features=2000000, ngram_range=(1, 3))\n",
        "\n",
        "lr = LogisticRegression()\n",
        "svm_clf = LinearSVC(random_state=0, tol=1e-5, dual=False)\n",
        "task_name = 'sub_twitter'\n",
        "tech_name = 'Back_trans'\n",
        "ratios = [0.2, 0.4, 0.6, 0.8, 1]\n",
        "test_list = [\n",
        "            'train',\n",
        "            'oversampling',\n",
        "            'negative_aug',\n",
        "              ]\n",
        "clf = svm_clf\n",
        "print('svm clf')\n",
        "for ratio in ratios:\n",
        "  print('*' * 50)\n",
        "  print('ratio', ratio)\n",
        "  \n",
        "  execute(test_list, ratio, task_name, tech_name, tvec, clf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "svm clf\n",
            "**************************************************\n",
            "ratio 0.2\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6490\n",
            "precision: 0.5792\n",
            "recall: 0.4920\n",
            "f1 score: 0.4939\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6426\n",
            "precision: 0.5481\n",
            "recall: 0.5143\n",
            "f1 score: 0.5216\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : negative_aug\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6308\n",
            "precision: 0.5375\n",
            "recall: 0.5542\n",
            "f1 score: 0.5430\n",
            "**************************************************\n",
            "ratio 0.4\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6615\n",
            "precision: 0.5903\n",
            "recall: 0.5053\n",
            "f1 score: 0.5105\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6568\n",
            "precision: 0.5702\n",
            "recall: 0.5320\n",
            "f1 score: 0.5420\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : negative_aug\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6452\n",
            "precision: 0.5520\n",
            "recall: 0.5646\n",
            "f1 score: 0.5570\n",
            "**************************************************\n",
            "ratio 0.6\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6680\n",
            "precision: 0.6033\n",
            "recall: 0.5185\n",
            "f1 score: 0.5281\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6625\n",
            "precision: 0.5803\n",
            "recall: 0.5475\n",
            "f1 score: 0.5576\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : negative_aug\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6513\n",
            "precision: 0.5588\n",
            "recall: 0.5701\n",
            "f1 score: 0.5635\n",
            "**************************************************\n",
            "ratio 0.8\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6719\n",
            "precision: 0.6171\n",
            "recall: 0.5270\n",
            "f1 score: 0.5400\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6668\n",
            "precision: 0.5891\n",
            "recall: 0.5527\n",
            "f1 score: 0.5642\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : negative_aug\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6576\n",
            "precision: 0.5661\n",
            "recall: 0.5682\n",
            "f1 score: 0.5669\n",
            "**************************************************\n",
            "ratio 1\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6787\n",
            "precision: 0.6169\n",
            "recall: 0.5337\n",
            "f1 score: 0.5466\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6748\n",
            "precision: 0.5950\n",
            "recall: 0.5687\n",
            "f1 score: 0.5780\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : negative_aug\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6665\n",
            "precision: 0.5730\n",
            "recall: 0.5656\n",
            "f1 score: 0.5687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woHUbhXb3r0m",
        "colab_type": "text"
      },
      "source": [
        "## BERT "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3Tye3Op3wyP",
        "colab_type": "text"
      },
      "source": [
        "#### SVM (test) new"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5YnOgGb_7dL",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a30f1b3-ac0b-41d6-a807-26f69eefcfa0"
      },
      "source": [
        "\n",
        "\n",
        "%%time\n",
        "\n",
        "tvec = TfidfVectorizer(stop_words=None, max_features=2000000, ngram_range=(1, 3))\n",
        "\n",
        "lr = LogisticRegression()\n",
        "svm_clf = LinearSVC(random_state=0, tol=1e-5, dual=False)\n",
        "print('svm')\n",
        "\n",
        "task_name = 'sub_twitter'\n",
        "tech_name = 'Bert_ data augmentation-master'\n",
        "clf = svm_clf\n",
        "ratios = [0.2, 0.4, 0.6, 0.8, 1]\n",
        "for ratio in ratios:\n",
        "  if ratio == 0.2:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(unfine)_0.2',\n",
        "      '3_rep_(unfine)_0.2',\n",
        "      '4_rep_(unfine)_0.2',\n",
        "      '5_rep_(unfine)_0.2']\n",
        "    print('<'*50)\n",
        "    print('ratio', ratio)\n",
        "  if ratio == 0.4:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(unfine)_0.4',\n",
        "      '3_rep_(unfine)_0.4',\n",
        "      '4_rep_(unfine)_0.4',\n",
        "      '5_rep_(unfine)_0.4']\n",
        "    print('<'*50)\n",
        "    print('ratio', ratio)\n",
        "\n",
        "  if ratio == 0.6:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(unfine)_0.6',\n",
        "      '3_rep_(unfine)_0.6',\n",
        "      '4_rep_(unfine)_0.6',\n",
        "      '5_rep_(unfine)_0.6']\n",
        "    print('<'*50)\n",
        "    print('ratio', ratio)\n",
        "\n",
        "  if ratio == 0.8:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(unfine)_0.8',\n",
        "      '3_rep_(unfine)_0.8',\n",
        "      '4_rep_(unfine)_0.8',\n",
        "      '5_rep_(unfine)_0.8']\n",
        "    print('<'*50)\n",
        "    print('ratio', ratio)\n",
        "\n",
        "  if ratio == 1:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(unfine)_1',\n",
        "      '3_rep_(unfine)_1',\n",
        "      '4_rep_(unfine)_1',\n",
        "      '5_rep_(unfine)_1']\n",
        "    print('<'*50)\n",
        "    print('ratio', ratio)\n",
        "\n",
        "  execute(test_list, ratio, task_name, tech_name, tvec, clf)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "svm\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "ratio 0.2\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6490\n",
            "precision: 0.5792\n",
            "recall: 0.4920\n",
            "f1 score: 0.4939\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6426\n",
            "precision: 0.5481\n",
            "recall: 0.5143\n",
            "f1 score: 0.5216\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_rep_(unfine)_0.2\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6346\n",
            "precision: 0.5367\n",
            "recall: 0.5368\n",
            "f1 score: 0.5355\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_rep_(unfine)_0.2\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6301\n",
            "precision: 0.5275\n",
            "recall: 0.5322\n",
            "f1 score: 0.5284\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_rep_(unfine)_0.2\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6327\n",
            "precision: 0.5293\n",
            "recall: 0.5270\n",
            "f1 score: 0.5269\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 5_rep_(unfine)_0.2\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6340\n",
            "precision: 0.5316\n",
            "recall: 0.5263\n",
            "f1 score: 0.5274\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "ratio 0.4\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6615\n",
            "precision: 0.5903\n",
            "recall: 0.5053\n",
            "f1 score: 0.5105\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6568\n",
            "precision: 0.5702\n",
            "recall: 0.5320\n",
            "f1 score: 0.5420\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_rep_(unfine)_0.4\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6469\n",
            "precision: 0.5476\n",
            "recall: 0.5490\n",
            "f1 score: 0.5476\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_rep_(unfine)_0.4\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6454\n",
            "precision: 0.5439\n",
            "recall: 0.5462\n",
            "f1 score: 0.5444\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_rep_(unfine)_0.4\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6481\n",
            "precision: 0.5470\n",
            "recall: 0.5417\n",
            "f1 score: 0.5434\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 5_rep_(unfine)_0.4\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6518\n",
            "precision: 0.5483\n",
            "recall: 0.5356\n",
            "f1 score: 0.5400\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "ratio 0.6\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6680\n",
            "precision: 0.6033\n",
            "recall: 0.5185\n",
            "f1 score: 0.5281\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6625\n",
            "precision: 0.5803\n",
            "recall: 0.5475\n",
            "f1 score: 0.5576\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_rep_(unfine)_0.6\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6497\n",
            "precision: 0.5542\n",
            "recall: 0.5629\n",
            "f1 score: 0.5574\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_rep_(unfine)_0.6\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6468\n",
            "precision: 0.5505\n",
            "recall: 0.5604\n",
            "f1 score: 0.5542\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_rep_(unfine)_0.6\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6526\n",
            "precision: 0.5552\n",
            "recall: 0.5562\n",
            "f1 score: 0.5551\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 5_rep_(unfine)_0.6\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6569\n",
            "precision: 0.5591\n",
            "recall: 0.5499\n",
            "f1 score: 0.5532\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "ratio 0.8\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6719\n",
            "precision: 0.6171\n",
            "recall: 0.5270\n",
            "f1 score: 0.5400\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6668\n",
            "precision: 0.5891\n",
            "recall: 0.5527\n",
            "f1 score: 0.5642\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_rep_(unfine)_0.8\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6573\n",
            "precision: 0.5680\n",
            "recall: 0.5786\n",
            "f1 score: 0.5723\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_rep_(unfine)_0.8\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6551\n",
            "precision: 0.5629\n",
            "recall: 0.5730\n",
            "f1 score: 0.5669\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_rep_(unfine)_0.8\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6581\n",
            "precision: 0.5648\n",
            "recall: 0.5645\n",
            "f1 score: 0.5641\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 5_rep_(unfine)_0.8\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6603\n",
            "precision: 0.5672\n",
            "recall: 0.5560\n",
            "f1 score: 0.5604\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "ratio 1\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6787\n",
            "precision: 0.6169\n",
            "recall: 0.5337\n",
            "f1 score: 0.5466\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6748\n",
            "precision: 0.5950\n",
            "recall: 0.5687\n",
            "f1 score: 0.5780\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_rep_(unfine)_1\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6652\n",
            "precision: 0.5774\n",
            "recall: 0.5889\n",
            "f1 score: 0.5819\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_rep_(unfine)_1\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6645\n",
            "precision: 0.5724\n",
            "recall: 0.5810\n",
            "f1 score: 0.5759\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_rep_(unfine)_1\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6669\n",
            "precision: 0.5749\n",
            "recall: 0.5745\n",
            "f1 score: 0.5742\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 5_rep_(unfine)_1\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6690\n",
            "precision: 0.5756\n",
            "recall: 0.5673\n",
            "f1 score: 0.5706\n",
            "CPU times: user 5min 29s, sys: 2min 16s, total: 7min 46s\n",
            "Wall time: 5min 6s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGyBUC2sYBTA",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuned BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDAIH9G9YJpN",
        "colab_type": "text"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gL2TVYRqfQl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a8913cc5-8fba-4649-d46c-21b3601b8469"
      },
      "source": [
        "\n",
        "\n",
        "%%time\n",
        "\n",
        "tvec = TfidfVectorizer(stop_words=None, max_features=2000000, ngram_range=(1, 3))\n",
        "\n",
        "lr = LogisticRegression()\n",
        "svm_clf = LinearSVC(random_state=0, tol=1e-5, dual=False)\n",
        "print('svm')\n",
        "\n",
        "task_name = 'sub_twitter'\n",
        "tech_name = 'Bert_ data augmentation-master'\n",
        "clf = svm_clf\n",
        "ratios = [0.2, 0.4, 0.6, 0.8, 1]\n",
        "for ratio in ratios:\n",
        "  if ratio == 0.2:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(tuned)_0.2',\n",
        "      '3_rep_(tuned)_0.2',\n",
        "      '4_rep_(tuned)_0.2',\n",
        "      '5_rep_(tuned)_0.2']\n",
        "    print('<'*50)\n",
        "    # print('ratio', ratio)\n",
        "  if ratio == 0.4:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(tuned)_0.4',\n",
        "      '3_rep_(tuned)_0.4',\n",
        "      '4_rep_(tuned)_0.4',\n",
        "      '5_rep_(tuned)_0.4']\n",
        "    print('<'*50)\n",
        "    # print('ratio', ratio)\n",
        "\n",
        "  if ratio == 0.6:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(tuned)_0.6',\n",
        "      '3_rep_(tuned)_0.6',\n",
        "      '4_rep_(tuned)_0.6',\n",
        "      '5_rep_(tuned)_0.6']\n",
        "    print('<'*50)\n",
        "    # print('ratio', ratio)\n",
        "\n",
        "  if ratio == 0.8:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(tuned)_0.8',\n",
        "      '3_rep_(tuned)_0.8',\n",
        "      '4_rep_(tuned)_0.8',\n",
        "      '5_rep_(tuned)_0.8']\n",
        "    print('<'*50)\n",
        "    # print('ratio', ratio)\n",
        "\n",
        "  if ratio == 1:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(tuned)_1',\n",
        "      '3_rep_(tuned)_1',\n",
        "      '4_rep_(tuned)_1',\n",
        "      '5_rep_(tuned)_1']\n",
        "\n",
        "    print('<'*50)\n",
        "    \n",
        "  execute(test_list, ratio, task_name, tech_name, tvec, clf)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "svm\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6490\n",
            "precision: 0.5792\n",
            "recall: 0.4920\n",
            "f1 score: 0.4939\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6426\n",
            "precision: 0.5481\n",
            "recall: 0.5143\n",
            "f1 score: 0.5216\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_rep_(tuned)_0.2\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6337\n",
            "precision: 0.5329\n",
            "recall: 0.5293\n",
            "f1 score: 0.5296\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_rep_(tuned)_0.2\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6336\n",
            "precision: 0.5333\n",
            "recall: 0.5304\n",
            "f1 score: 0.5305\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_rep_(tuned)_0.2\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6352\n",
            "precision: 0.5301\n",
            "recall: 0.5201\n",
            "f1 score: 0.5229\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 5_rep_(tuned)_0.2\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6360\n",
            "precision: 0.5329\n",
            "recall: 0.5214\n",
            "f1 score: 0.5248\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6615\n",
            "precision: 0.5903\n",
            "recall: 0.5053\n",
            "f1 score: 0.5105\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6568\n",
            "precision: 0.5702\n",
            "recall: 0.5320\n",
            "f1 score: 0.5420\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_rep_(tuned)_0.4\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6469\n",
            "precision: 0.5499\n",
            "recall: 0.5485\n",
            "f1 score: 0.5484\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_rep_(tuned)_0.4\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6452\n",
            "precision: 0.5475\n",
            "recall: 0.5492\n",
            "f1 score: 0.5478\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_rep_(tuned)_0.4\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6463\n",
            "precision: 0.5410\n",
            "recall: 0.5332\n",
            "f1 score: 0.5359\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 5_rep_(tuned)_0.4\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6541\n",
            "precision: 0.5581\n",
            "recall: 0.5387\n",
            "f1 score: 0.5453\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6680\n",
            "precision: 0.6033\n",
            "recall: 0.5185\n",
            "f1 score: 0.5281\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6625\n",
            "precision: 0.5803\n",
            "recall: 0.5475\n",
            "f1 score: 0.5576\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_rep_(tuned)_0.6\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6530\n",
            "precision: 0.5591\n",
            "recall: 0.5680\n",
            "f1 score: 0.5625\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_rep_(tuned)_0.6\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6502\n",
            "precision: 0.5574\n",
            "recall: 0.5654\n",
            "f1 score: 0.5606\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_rep_(tuned)_0.6\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6553\n",
            "precision: 0.5594\n",
            "recall: 0.5604\n",
            "f1 score: 0.5594\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 5_rep_(tuned)_0.6\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6581\n",
            "precision: 0.5612\n",
            "recall: 0.5506\n",
            "f1 score: 0.5546\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6719\n",
            "precision: 0.6171\n",
            "recall: 0.5270\n",
            "f1 score: 0.5400\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6668\n",
            "precision: 0.5891\n",
            "recall: 0.5527\n",
            "f1 score: 0.5642\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_rep_(tuned)_0.8\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6578\n",
            "precision: 0.5708\n",
            "recall: 0.5791\n",
            "f1 score: 0.5742\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_rep_(tuned)_0.8\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6590\n",
            "precision: 0.5717\n",
            "recall: 0.5800\n",
            "f1 score: 0.5751\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_rep_(tuned)_0.8\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6611\n",
            "precision: 0.5703\n",
            "recall: 0.5680\n",
            "f1 score: 0.5687\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 5_rep_(tuned)_0.8\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6644\n",
            "precision: 0.5762\n",
            "recall: 0.5614\n",
            "f1 score: 0.5672\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6787\n",
            "precision: 0.6169\n",
            "recall: 0.5337\n",
            "f1 score: 0.5466\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6748\n",
            "precision: 0.5950\n",
            "recall: 0.5687\n",
            "f1 score: 0.5780\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_rep_(tuned)_1\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6637\n",
            "precision: 0.5780\n",
            "recall: 0.5922\n",
            "f1 score: 0.5838\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_rep_(tuned)_1\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6625\n",
            "precision: 0.5749\n",
            "recall: 0.5893\n",
            "f1 score: 0.5807\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_rep_(tuned)_1\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6638\n",
            "precision: 0.5712\n",
            "recall: 0.5738\n",
            "f1 score: 0.5720\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 5_rep_(tuned)_1\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6683\n",
            "precision: 0.5792\n",
            "recall: 0.5708\n",
            "f1 score: 0.5741\n",
            "CPU times: user 5min 22s, sys: 2min 13s, total: 7min 35s\n",
            "Wall time: 4min 59s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_aZtmrIhJ2g",
        "colab_type": "text"
      },
      "source": [
        "## Fasttext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtGzvkCHhRFP",
        "colab_type": "text"
      },
      "source": [
        "### SVM test new"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRWP3UTThSbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "06c90ca9-47f3-4788-f748-4703fa10df85"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tvec = TfidfVectorizer(stop_words=None, max_features=1000000, ngram_range=(1, 3))\n",
        "\n",
        "lr = LogisticRegression()\n",
        "svm_clf = svm.SVC(kernel = 'linear')\n",
        "clf = svm_clf\n",
        "print('lr')\n",
        "\n",
        "task_name = 'sub_twitter'\n",
        "tech_name = 'Fasttext_augmentation'\n",
        "test_list = [\n",
        "      'train',\n",
        "      'oversampling',\n",
        "      '2_rep_(tuned)',\n",
        "      '3_rep_(tuned)',\n",
        "      # '4_rep_(tuned)',\n",
        "      \n",
        "      ]\n",
        "ratios = [0.2, 0.4, 0.6, 0.8, 1]\n",
        "for ratio in ratios:\n",
        "  print('ratio', ratio)\n",
        "  if ratio == 0.2:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(tuned)_0.2',\n",
        "      '3_rep_(tuned)_0.2',\n",
        "      '4_rep_(tuned)_0.2',\n",
        "      '5_rep_(tuned)_0.2']\n",
        "    print('<'*50)\n",
        "    # print('ratio', ratio)\n",
        "  if ratio == 0.4:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(tuned)_0.4',\n",
        "      '3_rep_(tuned)_0.4',\n",
        "      '4_rep_(tuned)_0.4',\n",
        "      '5_rep_(tuned)_0.4']\n",
        "    print('<'*50)\n",
        "    # print('ratio', ratio)\n",
        "\n",
        "  if ratio == 0.6:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(tuned)_0.6',\n",
        "      '3_rep_(tuned)_0.6',\n",
        "      '4_rep_(tuned)_0.6',\n",
        "      '5_rep_(tuned)_0.6']\n",
        "    print('<'*50)\n",
        "    # print('ratio', ratio)\n",
        "\n",
        "  if ratio == 0.8:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(tuned)_0.8',\n",
        "      '3_rep_(tuned)_0.8',\n",
        "      '4_rep_(tuned)_0.8',\n",
        "      '5_rep_(tuned)_0.8']\n",
        "    print('<'*50)\n",
        "    # print('ratio', ratio)\n",
        "\n",
        "  if ratio == 1:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_rep_(tuned)_1',\n",
        "      '3_rep_(tuned)_1',\n",
        "      '4_rep_(tuned)_1',\n",
        "      '5_rep_(tuned)_1']\n",
        "    print('<'*50)\n",
        "    # print('ratio', ratio)\n",
        "\n",
        "\n",
        "  \n",
        "  execute(test_list, ratio, task_name, tech_name, tvec, clf)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lr\n",
            "ratio 0.2\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6501\n",
            "precision: 0.5820\n",
            "recall: 0.4774\n",
            "f1 score: 0.4683\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6438\n",
            "precision: 0.5527\n",
            "recall: 0.5018\n",
            "f1 score: 0.5095\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_rep_(tuned)_0.2\n",
            "----------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-bc4feb53c88d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m   \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtech_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-c3543280c42c>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(test_list, ratio, task_name, tech_name, tkz, clf)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# DA and oversampling data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtech_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'oversampling'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtech_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-783320ed644a>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path_, FILE_NAME, ratio, tech_name)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mFILE_NAME\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'oversampling'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFILE_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mFILE_NAME\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtech_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Back_trans'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /content/drive/My Drive/Data/Fasttext_augmentation/aug_data/sub_twitter/2_rep_(tuned)_0.2.tsv does not exist: '/content/drive/My Drive/Data/Fasttext_augmentation/aug_data/sub_twitter/2_rep_(tuned)_0.2.tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNZ-2tl_hU0O",
        "colab_type": "text"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRp469Y3hYoY",
        "colab_type": "text"
      },
      "source": [
        "### SVM (test, new)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sudCx5DhabK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a60ecb7-7ecd-4ad1-c9f1-260a8da5ea9b"
      },
      "source": [
        "%%time\n",
        "clf = svm_clf\n",
        "test_list = [\n",
        "      # 'train',\n",
        "      # 'oversampling',\n",
        "      '2_del',\n",
        "      '3_del',\n",
        "      '4_del',\n",
        "      # '5_del',\n",
        "      \n",
        "      ]\n",
        "task_name = 'sub_twitter'\n",
        "tech_name = 'EDA'\n",
        "ratios = [0.2, 0.4, 0.6, 0.8, 1]\n",
        "for ratio in ratios:\n",
        "  print('ratio', ratio)\n",
        "  if ratio == 0.2:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_del_0.2',\n",
        "      '3_del_0.2',\n",
        "      '4_del_0.2',\n",
        "      ]\n",
        "    print('<'*50)\n",
        "    # print('ratio', ratio)\n",
        "  if ratio == 0.4:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_del_0.4',\n",
        "      '3_del_0.4',\n",
        "      '4_del_0.4',\n",
        "      ]\n",
        "    print('<'*50)\n",
        "    # print('ratio', ratio)\n",
        "\n",
        "  if ratio == 0.6:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_del_0.6',\n",
        "      '3_del_0.6',\n",
        "      '4_del_0.6',\n",
        "      ]\n",
        "    print('<'*50)\n",
        "    # print('ratio', ratio)\n",
        "\n",
        "  if ratio == 0.8:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_del_0.8',\n",
        "      '3_del_0.8',\n",
        "      '4_del_0.8',\n",
        "      ]\n",
        "    print('<'*50)\n",
        "    # print('ratio', ratio)\n",
        "\n",
        "  if ratio == 1:\n",
        "    test_list = ['train',\n",
        "                 'oversampling',\n",
        "                 '2_del_1',\n",
        "      '3_del_1',\n",
        "      '4_del_1',\n",
        "      ]\n",
        "\n",
        "    print('<'*50)\n",
        "  execute(test_list, ratio, task_name, tech_name, tvec, clf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ratio 0.2\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6501\n",
            "precision: 0.5820\n",
            "recall: 0.4774\n",
            "f1 score: 0.4683\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6438\n",
            "precision: 0.5527\n",
            "recall: 0.5018\n",
            "f1 score: 0.5095\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_del_0.2\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6347\n",
            "precision: 0.5321\n",
            "recall: 0.5250\n",
            "f1 score: 0.5279\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_del_0.2\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6364\n",
            "precision: 0.5354\n",
            "recall: 0.5263\n",
            "f1 score: 0.5299\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_del_0.2\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6359\n",
            "precision: 0.5348\n",
            "recall: 0.5284\n",
            "f1 score: 0.5310\n",
            "ratio 0.4\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6677\n",
            "precision: 0.6259\n",
            "recall: 0.4977\n",
            "f1 score: 0.4964\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6622\n",
            "precision: 0.5723\n",
            "recall: 0.5236\n",
            "f1 score: 0.5338\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_del_0.4\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6504\n",
            "precision: 0.5473\n",
            "recall: 0.5404\n",
            "f1 score: 0.5434\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_del_0.4\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6513\n",
            "precision: 0.5498\n",
            "recall: 0.5432\n",
            "f1 score: 0.5461\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_del_0.4\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6521\n",
            "precision: 0.5517\n",
            "recall: 0.5417\n",
            "f1 score: 0.5459\n",
            "ratio 0.6\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6744\n",
            "precision: 0.6299\n",
            "recall: 0.5094\n",
            "f1 score: 0.5135\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6662\n",
            "precision: 0.5785\n",
            "recall: 0.5359\n",
            "f1 score: 0.5470\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_del_0.6\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6570\n",
            "precision: 0.5628\n",
            "recall: 0.5647\n",
            "f1 score: 0.5637\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_del_0.6\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6573\n",
            "precision: 0.5596\n",
            "recall: 0.5593\n",
            "f1 score: 0.5594\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_del_0.6\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6590\n",
            "precision: 0.5623\n",
            "recall: 0.5579\n",
            "f1 score: 0.5599\n",
            "ratio 0.8\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6757\n",
            "precision: 0.6282\n",
            "recall: 0.5133\n",
            "f1 score: 0.5197\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6702\n",
            "precision: 0.5914\n",
            "recall: 0.5463\n",
            "f1 score: 0.5593\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_del_0.8\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6617\n",
            "precision: 0.5707\n",
            "recall: 0.5736\n",
            "f1 score: 0.5721\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_del_0.8\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6606\n",
            "precision: 0.5671\n",
            "recall: 0.5667\n",
            "f1 score: 0.5669\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_del_0.8\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6645\n",
            "precision: 0.5721\n",
            "recall: 0.5662\n",
            "f1 score: 0.5689\n",
            "ratio 1\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : train\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6833\n",
            "precision: 0.6221\n",
            "recall: 0.5215\n",
            "f1 score: 0.5288\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : oversampling\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6796\n",
            "precision: 0.6026\n",
            "recall: 0.5605\n",
            "f1 score: 0.5737\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 2_del_1\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6712\n",
            "precision: 0.5810\n",
            "recall: 0.5861\n",
            "f1 score: 0.5833\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 3_del_1\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6713\n",
            "precision: 0.5799\n",
            "recall: 0.5821\n",
            "f1 score: 0.5809\n",
            "----------------------------------------------------------------------\n",
            "Start to make classification use the data : 4_del_1\n",
            "----------------------------------------------------------------------\n",
            "accuracy: 0.6719\n",
            "precision: 0.5807\n",
            "recall: 0.5768\n",
            "f1 score: 0.5786\n",
            "CPU times: user 6h 18min 14s, sys: 9.56 s, total: 6h 18min 23s\n",
            "Wall time: 6h 18min 52s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}